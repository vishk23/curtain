{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oracle Table Diagnostics v2\n",
    "## Auto-discovers columns first, then checks for issues\n",
    "\n",
    "Fixes: Discovers actual column names before running checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONNECTION CONFIGURATION - UPDATE THESE VALUES\n",
    "# ============================================================\n",
    "\n",
    "ORACLE_HOST = \"your-oracle-server\"  \n",
    "ORACLE_PORT = \"1521\"                \n",
    "ORACLE_SERVICE = \"COCCDM\"           \n",
    "ORACLE_USER = \"your_username\"       \n",
    "ORACLE_PASSWORD = \"your_password\"   \n",
    "\n",
    "ORACLE_DSN = f\"{ORACLE_HOST}:{ORACLE_PORT}/{ORACLE_SERVICE}\"\n",
    "SCHEMA = \"COCCDM\"\n",
    "\n",
    "TABLES = ['WH_LOANS', 'WH_ACCTCOMMON', 'WH_ACCT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import oracledb\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Comment out next line for thin mode (no Oracle client)\n",
    "# oracledb.init_oracle_client()\n",
    "\n",
    "print(f\"Connecting to {ORACLE_DSN}...\")\n",
    "conn = oracledb.connect(user=ORACLE_USER, password=ORACLE_PASSWORD, dsn=ORACLE_DSN)\n",
    "print(f\"Connected! Oracle version: {conn.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DISCOVER SCHEMA - What columns actually exist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all columns for each table\n",
    "table_schemas = {}\n",
    "\n",
    "for table in TABLES:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{table} - COLUMN SCHEMA\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    df = pd.read_sql(f\"\"\"\n",
    "        SELECT column_name, data_type, data_precision, data_scale, nullable\n",
    "        FROM all_tab_columns\n",
    "        WHERE owner = '{SCHEMA}'\n",
    "          AND table_name = '{table}'\n",
    "        ORDER BY column_id\n",
    "    \"\"\", conn)\n",
    "    \n",
    "    table_schemas[table] = df\n",
    "    print(df.to_string())\n",
    "    \n",
    "    # Summary\n",
    "    date_cols = df[df['DATA_TYPE'] == 'DATE']['COLUMN_NAME'].tolist()\n",
    "    num_cols = df[df['DATA_TYPE'] == 'NUMBER']['COLUMN_NAME'].tolist()\n",
    "    print(f\"\\nDATE columns: {date_cols}\")\n",
    "    print(f\"NUMBER columns: {num_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ROW COUNTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TABLE ROW COUNTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for table in TABLES:\n",
    "    try:\n",
    "        df = pd.read_sql(f\"SELECT COUNT(*) as cnt FROM {SCHEMA}.{table}\", conn)\n",
    "        print(f\"{table}: {df['CNT'].iloc[0]:,} rows\")\n",
    "    except Exception as e:\n",
    "        print(f\"{table}: ERROR - {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CHECK ALL DATE COLUMNS FOR INVALID VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check every DATE column in every table for invalid values\n",
    "invalid_dates_found = []\n",
    "\n",
    "for table in TABLES:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{table} - DATE COLUMN VALIDATION\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Get DATE columns for this table\n",
    "    date_cols = table_schemas[table][table_schemas[table]['DATA_TYPE'] == 'DATE']['COLUMN_NAME'].tolist()\n",
    "    \n",
    "    if not date_cols:\n",
    "        print(\"No DATE columns found\")\n",
    "        continue\n",
    "    \n",
    "    for col in date_cols:\n",
    "        try:\n",
    "            # Check for dates outside reasonable range\n",
    "            df = pd.read_sql(f\"\"\"\n",
    "                SELECT \n",
    "                    COUNT(*) as total_rows,\n",
    "                    COUNT({col}) as non_null,\n",
    "                    MIN({col}) as min_date,\n",
    "                    MAX({col}) as max_date,\n",
    "                    SUM(CASE WHEN EXTRACT(YEAR FROM {col}) < 1900 THEN 1 ELSE 0 END) as before_1900,\n",
    "                    SUM(CASE WHEN EXTRACT(YEAR FROM {col}) > 2100 THEN 1 ELSE 0 END) as after_2100,\n",
    "                    SUM(CASE WHEN EXTRACT(YEAR FROM {col}) < 1 OR EXTRACT(YEAR FROM {col}) > 9999 THEN 1 ELSE 0 END) as outside_dotnet\n",
    "                FROM {SCHEMA}.{table}\n",
    "            \"\"\", conn)\n",
    "            \n",
    "            before_1900 = df['BEFORE_1900'].iloc[0] or 0\n",
    "            after_2100 = df['AFTER_2100'].iloc[0] or 0\n",
    "            outside_dotnet = df['OUTSIDE_DOTNET'].iloc[0] or 0\n",
    "            min_dt = df['MIN_DATE'].iloc[0]\n",
    "            max_dt = df['MAX_DATE'].iloc[0]\n",
    "            \n",
    "            status = \"OK\"\n",
    "            if outside_dotnet > 0:\n",
    "                status = f\"*** CRITICAL: {outside_dotnet} outside .NET range ***\"\n",
    "                invalid_dates_found.append((table, col, outside_dotnet, min_dt, max_dt))\n",
    "            elif before_1900 > 0 or after_2100 > 0:\n",
    "                status = f\"WARN: {before_1900} before 1900, {after_2100} after 2100\"\n",
    "                invalid_dates_found.append((table, col, before_1900 + after_2100, min_dt, max_dt))\n",
    "            \n",
    "            print(f\"{col}: min={min_dt}, max={max_dt} - {status}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{col}: ERROR - {str(e)[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of problematic date columns\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROBLEMATIC DATE COLUMNS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if invalid_dates_found:\n",
    "    for table, col, count, min_dt, max_dt in invalid_dates_found:\n",
    "        print(f\"\\n{table}.{col}:\")\n",
    "        print(f\"  Invalid rows: {count}\")\n",
    "        print(f\"  Min date: {min_dt}\")\n",
    "        print(f\"  Max date: {max_dt}\")\n",
    "else:\n",
    "    print(\"No problematic date columns found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SAMPLE BAD DATE ROWS (if any found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample rows with bad dates\n",
    "for table, col, count, min_dt, max_dt in invalid_dates_found:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{table} - SAMPLE ROWS WITH BAD {col}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        # Get first few columns + the bad date column\n",
    "        df = pd.read_sql(f\"\"\"\n",
    "            SELECT *\n",
    "            FROM {SCHEMA}.{table}\n",
    "            WHERE EXTRACT(YEAR FROM {col}) < 1900 \n",
    "               OR EXTRACT(YEAR FROM {col}) > 2100\n",
    "            FETCH FIRST 10 ROWS ONLY\n",
    "        \"\"\", conn)\n",
    "        print(df.to_string())\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. NUMBER COLUMN PRECISION CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check NUMBER columns for precision issues\n",
    "for table in TABLES:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{table} - NUMBER COLUMNS WITH NULL PRECISION (unlimited)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    schema_df = table_schemas[table]\n",
    "    num_cols = schema_df[(schema_df['DATA_TYPE'] == 'NUMBER') & (schema_df['DATA_PRECISION'].isna())]\n",
    "    \n",
    "    if len(num_cols) > 0:\n",
    "        print(\"These columns have unlimited precision (potential overflow risk):\")\n",
    "        print(num_cols[['COLUMN_NAME', 'DATA_TYPE', 'DATA_PRECISION', 'DATA_SCALE']].to_string())\n",
    "    else:\n",
    "        print(\"All NUMBER columns have defined precision - OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check actual value ranges for NUMBER columns\n",
    "for table in TABLES:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{table} - NUMBER VALUE RANGES\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    num_cols = table_schemas[table][table_schemas[table]['DATA_TYPE'] == 'NUMBER']['COLUMN_NAME'].tolist()\n",
    "    \n",
    "    for col in num_cols[:10]:  # Limit to first 10 to avoid long runtime\n",
    "        try:\n",
    "            df = pd.read_sql(f\"\"\"\n",
    "                SELECT MIN({col}) as min_val, MAX({col}) as max_val\n",
    "                FROM {SCHEMA}.{table}\n",
    "                WHERE {col} IS NOT NULL\n",
    "            \"\"\", conn)\n",
    "            min_v = df['MIN_VAL'].iloc[0]\n",
    "            max_v = df['MAX_VAL'].iloc[0]\n",
    "            print(f\"{col}: min={min_v}, max={max_v}\")\n",
    "        except Exception as e:\n",
    "            print(f\"{col}: ERROR - {str(e)[:50]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. WH_LOANS SPECIFIC - DEEP DIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WH_LOANS deep dive - check all columns\n",
    "print(\"WH_LOANS - FULL COLUMN LIST WITH SAMPLE VALUES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    df = pd.read_sql(f\"\"\"\n",
    "        SELECT * FROM {SCHEMA}.WH_LOANS\n",
    "        FETCH FIRST 5 ROWS ONLY\n",
    "    \"\"\", conn)\n",
    "    print(f\"\\nColumns ({len(df.columns)}):\")\n",
    "    for i, col in enumerate(df.columns):\n",
    "        print(f\"  {i+1}. {col}\")\n",
    "    print(\"\\nSample data:\")\n",
    "    print(df.T)  # Transpose for readability\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. FINAL SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DIAGNOSTIC COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nRun at: {datetime.now()}\")\n",
    "\n",
    "print(\"\\n--- PROBLEMATIC DATE COLUMNS ---\")\n",
    "if invalid_dates_found:\n",
    "    for table, col, count, min_dt, max_dt in invalid_dates_found:\n",
    "        print(f\"  {table}.{col}: {count} bad rows (min={min_dt}, max={max_dt})\")\n",
    "else:\n",
    "    print(\"  None found\")\n",
    "\n",
    "print(\"\\n--- NEXT STEPS ---\")\n",
    "print(\"1. Push this notebook output back to repo\")\n",
    "print(\"2. Use output to build safe CopyJob queries\")\n",
    "print(\"3. Apply CASE WHEN to handle bad date columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()\n",
    "print(\"Connection closed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
